#!/bin/bash

# Set environment variables
export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7

# Run training
torchrun \
    --nproc_per_node=8 \
    train.py \
    --model resnet50 \
    --batch-size 32 \
    --epochs 90 \
    --opt adamw \
    --lr 0.001 \
    --weight-decay 0.05 \
    --lr-scheduler cosineannealinglr \
    --lr-warmup-epochs 5 \
    --lr-warmup-method linear \
    --output-dir ./output \
    --resume ./output/checkpoint.pth 
    --amp

# Make script executable
chmod +x run_training.sh

####################################################################################
####################################################################################

Monitor GPU Usage
In a separate terminal:
watch nvidia-smi

####################################################################################
####################################################################################

Common Issues and Solutions
    1. Out of Memory (OOM)
       # Reduce batch size
       torchrun --nproc_per_node=8 train.py --model resnet50 --batch-size 16

    2. GPU Not Found
       # Check CUDA is available
       python3 -c "import torch; print(torch.cuda.is_available())"

       # Check visible GPUs
       echo $CUDA_VISIBLE_DEVICES

    3. Process Hanging
       # Kill all Python processes
       pkill python

       # Or more specifically
       pkill -f train.py

####################################################################################
####################################################################################
